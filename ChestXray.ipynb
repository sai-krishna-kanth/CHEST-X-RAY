#%%
from google.colab import drive
drive.mount('/content/drive')
#%%
from zipfile import ZipFile
file_name = "/content/drive/My Drive/DATASETS/chest-xray-pneumonia.zip"
with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print('Done')
#%%
from zipfile import ZipFile
file_name = "/content/chest_xray.zip"
with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print('Done')
#%%
import matplotlib.pyplot as plt
im = plt.imread('/content/chest_xray/train/NORMAL/IM-0115-0001.jpeg')
plt.imshow(im)
plt.show()
#%%
import matplotlib.pyplot as plt
im = plt.imread('/content/chest_xray/train/PNEUMONIA/person1000_bacteria_2931.jpeg')
plt.imshow(im)
plt.show()
#%%
import os
import pandas as pd
filenames = os.listdir("/content/chest_xray/train/NORMAL")
categories = []
for filename in filenames:
    category = filename.split('-')[0]
    if category == 'IM':
        categories.append(1)
    else:
        categories.append(0)

df1= pd.DataFrame({
    'filename': filenames,
    'category': categories
})
#%%
import os
import pandas as pd
filenames = os.listdir("/content/chest_xray/train/PNEUMONIA")
categories = []
for filename in filenames:
    category = filename.split('1')[0]
    if category == 'person':
        categories.append(0)
    else:
        categories.append(1)

df2= pd.DataFrame({
    'filename': filenames,
    'category': categories
})
#%%

import random
sam = random.choice(filenames)
abc = filename.split('-')[0]
if abc == 'IM':
	image = plt.imread('/content/chest_xray/train/NORMAL'+sam)
else:
	image = plt.imread('/content/chest_xray/train/PNEUMONIA/'+sam)
plt.imshow(image)
#%%
%matplotlib inline
import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg #The image module supports basic image loading, rescaling and display operations.

train_cat_fnames = os.listdir("/content/chest_xray/train/NORMAL")
train_dog_fnames = os.listdir("/content/chest_xray/train/PNEUMONIA")
nrows = 3
ncols = 3
pic_index = 0
pic_index += 4
next_cat_pix = [os.path.join("/content/chest_xray/train/NORMAL", fname)
               for fname in train_cat_fnames[pic_index-4:pic_index]]
next_dog_pix = [os.path.join("/content/chest_xray/train/PNEUMONIA", fname)
               for fname in train_dog_fnames[pic_index-4:pic_index]]
fig=plt.gcf()
fig.set_size_inches(ncols*4,nrows*4)
for i, img_path in enumerate(next_cat_pix+next_dog_pix):
  sp = plt.subplot(nrows, ncols, i + 1)
  img = mpimg.imread(img_path)
  plt.imshow(img)

plt.show()

#%%
from tensorflow.keras import layers
from tensorflow.keras import Model
from tensorflow.keras.optimizers import RMSprop

img_input = layers.Input(shape=(150, 150, 3))

x = layers.Conv2D(16, 3, activation='relu')(img_input)
x = layers.MaxPooling2D(2)(x)

x = layers.Conv2D(32, 3, activation='relu')(x)
x = layers.MaxPooling2D(2)(x)

x = layers.Convolution2D(64, 3, activation='relu')(x)
x = layers.MaxPooling2D(2)(x)

x = layers.Flatten()(x)

x = layers.Dense(512, activation='relu')(x)

x = layers.Dropout(0.5)(x)

output = layers.Dense(1, activation='sigmoid')(x)

model = Model(img_input, output)
model.summary()


#%%
from tensorflow.keras.preprocessing.image import ImageDataGenerator
# All images will be rescaled by 1./255
train_data = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,)

test_data = ImageDataGenerator(rescale=1./255)

train_generator = train_data.flow_from_directory(
        "/content/chest_xray/train", 
        target_size=(150, 150),  # All images will be resized to 150x150
        batch_size=10,
        class_mode='binary')

validation_generator = test_data.flow_from_directory(
        "/content/chest_xray/test",
        target_size=(150, 150),
        batch_size=10,
        class_mode='binary')

test_generator = test_data.flow_from_directory(
        "/content/chest_xray/val",
        target_size=(150, 150),
        batch_size=10,
        class_mode='binary')

#%%
!wget --no-check-certificate \
    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \
    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5
#%%
from tensorflow.keras.applications.inception_v3 import InceptionV3

local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'
pre_trained_model = InceptionV3(
    input_shape=(150, 150, 3), include_top=False, weights=None)
pre_trained_model.load_weights(local_weights_file)
#%%
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
# add a global spatial average pooling layer
x = pre_trained_model.output
x = GlobalAveragePooling2D()(x)
# let's add a fully-connected layer
# this is the model we made and trained
x = Dense(1024, activation='relu')(x)
# and a logistic layer -- let's say we have 200 classes
predictions = Dense(1, activation='sigmoid')(x)
#%%
# this is the model we will train
model = Model(inputs=pre_trained_model.input, outputs=predictions)

# first: train only the top layers (which were randomly initialized)
# i.e. freeze all convolutional InceptionV3 layers
for layer in pre_trained_model.layers:
    layer.trainable = False

# compile the model (should be done *after* setting layers to non-trainable)
model.compile(loss='binary_crossentropy',
             optimizer=RMSprop(lr=0.01),
             metrics=['acc'])
model.summary()
#%%
# train the model on the new data for a few epochs
model.fit_generator(train_generator,
     steps_per_epoch=500,  # 2000 images = batch_size * steps
     epochs=5,
     validation_data=validation_generator,
     validation_steps=62,  # 1000 images = batch_size * steps
     verbose=2)


# let's visualize layer names and layer indices to see how many layers
last_layer = pre_trained_model.get_layer('mixed5')
last_output = last_layer.output
x = layers.Flatten()(last_output)
x = layers.Dense(1024, activation='relu')(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(1, activation='sigmoid')(x)

model = Model(pre_trained_model.input, x)
model.compile(loss='binary_crossentropy',
            optimizer=RMSprop(lr=0.0001),
            metrics=['acc'])
# train the model on the new data for a few epochs
model.fit_generator(train_generator,
     steps_per_epoch=500,  # 2000 images = batch_size * steps
     epochs=15,
     validation_data=validation_generator,
     validation_steps=62,  # 1000 images = batch_size * steps
     verbose=2)
#%%
import tensorflow
model1 = tensorflow.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, 
                                                         weights='imagenet', 
                                                         input_shape=(150,150,3))
#%%
model1.summary()
#%%
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D,Dropout
# add a global spatial average pooling layer
for layer in pre_trained_model.layers:
    layer.trainable = False
    
last_layer = model1.get_layer("mixed_7a")
output = last_layer.output


x = GlobalAveragePooling2D()(output)
# let's add a fully-connected layer
x = Dense(1024, activation='relu')(x)

x = Dropout(0.5)(x)
# and a logistic layer -- let's say we have 200 classes
predictions = Dense(1, activation='sigmoid')(x)

model = Model(inputs=model1.input, outputs=predictions)
#%%
from tensorflow.keras.optimizers import SGD
# compile the model (should be done *after* setting layers to non-trainable)
model.compile(loss='binary_crossentropy',
             optimizer=SGD(lr=0.001, momentum=0.9, decay=0.01, nesterov=True),
             metrics=['acc'])
#%%
history = model.fit_generator(
      train_generator,
      steps_per_epoch=500,
      epochs=10,
      validation_data=validation_generator,
      validation_steps=62,
      verbose=2)
#%%
from tensorflow.keras.applications.vgg16 import VGG16

trained1_model = VGG16(include_top=False, 
                      weights="imagenet", 
                      input_shape=(150,150,3))
#%%
trained1_model.summary()
#%%
from tensorflow.keras import Model,layers

    
last_layer = trained1_model.get_layer("block5_conv3")
output = last_layer.output

x = layers.GlobalAveragePooling2D()(output)
x = layers.Dense(1024, activation='relu')(x)
x = layers.Dropout(0.5)(x)
y = layers.Dense(1, activation='sigmoid')(x)

model = Model(trained1_model.input, y)

#%%
flag = False

for layer in trained1_model.layers:
  if flag:
    layer.trainable = True
  if layer.name == 'block4_conv3':
    flag = True
#%%
from tensorflow.keras.optimizers import SGD


model.compile(loss='binary_crossentropy',
              optimizer=SGD(
                  lr=0.0001, 
                  momentum=0.9),
              metrics=['acc'])
#%%
X = model.fit_generator(
      train_generator,
      steps_per_epoch=100,
      epochs=20,
      validation_data=validation_generator,
      validation_steps=25,
      verbose=2)
#%%
import sys
from matplotlib import pyplot
# plot loss
pyplot.subplot(211)
pyplot.title('Cross Entropy Loss')
pyplot.plot(X.history['loss'], color='blue', label='train')
pyplot.plot(X.history['val_loss'], color='orange', label='test')
# plot accuracy
pyplot.subplot(212)
pyplot.title('Classification Accuracy')
pyplot.plot(X.history['acc'], color='blue', label='train')
pyplot.plot(X.history['val_acc'], color='orange', label='test')
pyplot.show()
#%%
X = model.fit_generator(
      train_generator,
      steps_per_epoch=100,
      epochs=20,
      validation_data=validation_generator,
      validation_steps=25,
      verbose=2)
#%%
import sys
from matplotlib import pyplot
# plot loss
pyplot.subplot(211)
pyplot.title('Cross Entropy Loss')
pyplot.plot(X.history['loss'], color='blue', label='train')
pyplot.plot(X.history['val_loss'], color='orange', label='test')
# plot accuracy
pyplot.subplot(212)
pyplot.title('Classification Accuracy')
pyplot.plot(X.history['acc'], color='blue', label='train')
pyplot.plot(X.history['val_acc'], color='orange', label='test')
pyplot.show()
#%%
